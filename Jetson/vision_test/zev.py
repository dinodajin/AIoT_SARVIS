# -*- coding: utf-8 -*-
import cv2
import numpy as np
import socket
import time
from insightface.app import FaceAnalysis

# --- ì„¤ì • ---
RPI_IP = "172.20.10.7"
RPI_PORT = 5005
sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)

# [í•µì‹¬] Jetson CSI ì¹´ë©”ë¼ìš© GStreamer íŒŒì´í”„ë¼ì¸ ë¬¸ìì—´ ìƒì„± í•¨ìˆ˜
def gstreamer_pipeline(
    sensor_id=0,
    capture_width=1280,
    capture_height=720,
    display_width=640,
    display_height=480,
    framerate=30,
    flip_method=0,
):
    return (
        "nvarguscamerasrc sensor-id=%d ! "
        "video/x-raw(memory:NVMM), width=(int)%d, height=(int)%d, format=(string)NV12, framerate=(fraction)%d/1 ! "
        "nvvidconv flip-method=%d ! "
        "video/x-raw, width=(int)%d, height=(int)%d, format=(string)BGRx ! "
        "videoconvert ! "
        "video/x-raw, format=(string)BGR ! appsink"
        % (
            sensor_id,
            capture_width,
            capture_height,
            framerate,
            flip_method,
            display_width,
            display_height,
        )
    )

class SarvisFaceTracker:
    def __init__(self):
        # ì†ë„ë¥¼ ìœ„í•´ detection ëª¨ë“ˆë§Œ ë¡œë“œ & í•´ìƒë„ 640x640 (ì¸ì‹ë¥  í™•ë³´)
        self.app = FaceAnalysis(
            name='buffalo_sc',
            providers=['CUDAExecutionProvider', 'CPUExecutionProvider'],
            allowed_modules=['detection']
        )
        self.app.prepare(ctx_id=0, det_size=(640, 640))
        print(">> InsightFace: Model Loaded.")

    def get_tracking_data(self, frame):
        # BGR -> RGB ë³€í™˜ (ì¸ì‹ë¥  í–¥ìƒ)
        img_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        faces = self.app.get(img_rgb)
        
        if not faces: return None

        # ê°€ì¥ í° ì–¼êµ´(ê°€ì¥ ê°€ê¹Œìš´ ì‚¬ëŒ) ì„ íƒ
        target_face = max(faces, key=lambda f: (f.bbox[2] - f.bbox[0]) * (f.bbox[3] - f.bbox[1]))

        h, w, _ = frame.shape
        center_screen = (w // 2, h // 2)
        lm = target_face.kps
        
        # ì¢Œí‘œ ê³„ì‚°
        offset_x = (int(lm[2][0]) - center_screen[0]) / (w / 2)
        offset_y = (int(lm[2][1]) - center_screen[1]) / (h / 2)
        
        bbox = target_face.bbox.astype(int)
        face_area = (bbox[2] - bbox[0]) * (bbox[3] - bbox[1])
        dist_z = ((w * h) / (face_area + 1e-6)) * 0.05
        
        dist_l = np.linalg.norm(lm[2] - lm[0])
        dist_r = np.linalg.norm(lm[2] - lm[1])
        yaw = (dist_r - dist_l) / (dist_r + dist_l + 1e-6) * 100
        
        eye_y = (lm[0][1] + lm[1][1]) / 2
        mouth_y = (lm[3][1] + lm[4][1]) / 2
        pitch = ((lm[2][1] - eye_y) / (mouth_y - eye_y + 1e-6) - 0.5) * 100
        
        print(f"Target: {int(face_area)}px | X:{offset_x:.2f} Y:{offset_y:.2f} Z:{dist_z:.2f}       ", end='\r')
        return f"FACE:{offset_x:.3f},{offset_y:.3f},{dist_z:.3f},{yaw:.1f},{pitch:.1f}"

if __name__ == "__main__":
    tracker = SarvisFaceTracker()
    
    # ---------------------------------------------------------
    # [ì¹´ë©”ë¼ ì—°ê²° ì‹œë„ ë¡œì§]
    # ---------------------------------------------------------
    cap = None
    
    # 1. CSI ì¹´ë©”ë¼ (GStreamer) ì‹œë„
    print("Trying CSI Camera (GStreamer)...")
    try:
        cap = cv2.VideoCapture(gstreamer_pipeline(flip_method=0), cv2.CAP_GSTREAMER)
    except Exception as e:
        print(f"GStreamer Error: {e}")

    # 2. CSI ì‹¤íŒ¨ ì‹œ USB ì¹´ë©”ë¼ (V4L2) ì‹œë„
    if not cap or not cap.isOpened():
        print("CSI failed. Trying USB Camera (/dev/video0)...")
        cap = cv2.VideoCapture(0, cv2.CAP_V4L2)
    
    # 3. video0 ì‹¤íŒ¨ ì‹œ video1 ì‹œë„
    if not cap or not cap.isOpened():
        print("video0 failed. Trying USB Camera (/dev/video1)...")
        cap = cv2.VideoCapture(1, cv2.CAP_V4L2)

    # ìµœì¢… í™•ì¸
    if not cap or not cap.isOpened():
        print("\nğŸš¨ CRITICAL ERROR: Could not open any camera!")
        print("Check hardware connection and verify 'ls -l /dev/video*' again.")
        exit()

    print("\nâœ… Camera Opened Successfully! Starting Loop...")
    
    # í•´ìƒë„ ì„¤ì • (USB ì¹´ë©”ë¼ì¼ ê²½ìš°ì—ë§Œ ì ìš©ë¨)
    cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)
    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)

    try:
        while True:
            ret, frame = cap.read()
            if not ret: 
                print("Failed to grab frame (ret=False)")
                time.sleep(1)
                continue
            
            data_string = tracker.get_tracking_data(frame)

            if data_string:
                sock.sendto(data_string.encode(), (RPI_IP, RPI_PORT))
            else:
                sock.sendto(b"NOFACE", (RPI_IP, RPI_PORT))
                print(f"Searching... (NOFACE)                    ", end='\r')
            
            time.sleep(0.01)

    except KeyboardInterrupt:
        print("\nStop.")
    finally:
        if cap: cap.release()
        cv2.destroyAllWindows()
